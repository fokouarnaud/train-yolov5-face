#!/usr/bin/env python3
"""
Script de test rapide pour valider les modules GD optimisÃ©s
Utiliser ce script pour tester avant l'entraÃ®nement complet
"""

import os
import sys
import torch
import torch.nn as nn
from pathlib import Path

def test_gd_modules():
    """Test des modules GD optimisÃ©s en mÃ©moire"""
    print("ğŸ§ª Test des modules GD optimisÃ©s pour la mÃ©moire...")
    
    # Ajouter le chemin YOLOv5-Face
    from config import DEFAULT_PATHS
    yolo_path = DEFAULT_PATHS["yolo_dir"]
    if yolo_path not in sys.path:
        sys.path.append(yolo_path)
    
    try:
        # Import des modules
        from models.gd import GDFusion, AttentionFusion, TransformerFusion
        print("   âœ… Import des modules GD rÃ©ussi")
        
        # ParamÃ¨tres de test
        batch_size = 2  # Petit batch pour test
        channels = 128
        height, width = 32, 32  # Petite rÃ©solution
        
        # CrÃ©er tenseur de test
        test_tensor = torch.randn(batch_size, channels, height, width)
        if torch.cuda.is_available():
            test_tensor = test_tensor.cuda()
            print(f"   ğŸ“± GPU: {torch.cuda.get_device_name()}")
        
        print(f"   ğŸ“Š Tensor test: {test_tensor.shape}")
        
        # Test 1: AttentionFusion optimisÃ©
        print("\n   ğŸ” Test AttentionFusion optimisÃ©...")
        attn_fusion = AttentionFusion(channels)
        if torch.cuda.is_available():
            attn_fusion = attn_fusion.cuda()
        
        # Forward pass
        output_attn = attn_fusion((test_tensor, test_tensor))
        print(f"      âœ… Input: {test_tensor.shape} â†’ Output: {output_attn.shape}")
        
        # VÃ©rifier la mÃ©moire utilisÃ©e
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1e6  # MB
            print(f"      ğŸ“ˆ MÃ©moire utilisÃ©e: {memory_used:.1f} MB")
        
        # Test 2: TransformerFusion optimisÃ©
        print("\n   ğŸ”„ Test TransformerFusion optimisÃ©...")
        trans_fusion = TransformerFusion(channels)
        if torch.cuda.is_available():
            trans_fusion = trans_fusion.cuda()
        
        # Forward pass
        output_trans = trans_fusion((test_tensor, test_tensor))
        print(f"      âœ… Input: {test_tensor.shape} â†’ Output: {output_trans.shape}")
        
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1e6  # MB
            print(f"      ğŸ“ˆ MÃ©moire utilisÃ©e: {memory_used:.1f} MB")
        
        # Test 3: GDFusion complet
        print("\n   ğŸ¯ Test GDFusion avec attention...")
        gd_fusion_attn = GDFusion(channels, channels, 'attention')
        if torch.cuda.is_available():
            gd_fusion_attn = gd_fusion_attn.cuda()
        
        output_gd_attn = gd_fusion_attn(test_tensor)
        print(f"      âœ… Input: {test_tensor.shape} â†’ Output: {output_gd_attn.shape}")
        
        print("\n   ğŸ¯ Test GDFusion avec transformer...")
        gd_fusion_trans = GDFusion(channels, channels, 'transformer')
        if torch.cuda.is_available():
            gd_fusion_trans = gd_fusion_trans.cuda()
        
        output_gd_trans = gd_fusion_trans(test_tensor)
        print(f"      âœ… Input: {test_tensor.shape} â†’ Output: {output_gd_trans.shape}")
        
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1e6  # MB
            print(f"      ğŸ“ˆ MÃ©moire totale utilisÃ©e: {memory_used:.1f} MB")
            
            # Nettoyer la mÃ©moire
            torch.cuda.empty_cache()
        
        print("\nâœ… Tous les tests des modules GD ont rÃ©ussi!")
        return True
        
    except Exception as e:
        print(f"\nâŒ Erreur lors du test des modules GD: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_model_loading():
    """Test de chargement du modÃ¨le ADYOLOv5"""
    print("\nğŸ—ï¸ Test de chargement du modÃ¨le ADYOLOv5...")
    
    from config import DEFAULT_PATHS
    yolo_path = DEFAULT_PATHS["yolo_dir"]
    sys.path.append(yolo_path)
    
    try:
        from models.yolo import Model
        
        # Tester le chargement du fichier YAML
        yaml_path = f"{yolo_path}/models/adyolov5s.yaml"
        if os.path.exists(yaml_path):
            print(f"   âœ… Fichier YAML trouvÃ©: {yaml_path}")
            
            # Charger le modÃ¨le (sans poids prÃ©-entraÃ®nÃ©s pour test rapide)
            model = Model(yaml_path, ch=3, nc=1)  # 1 classe pour dÃ©tection de visage
            
            # Test avec un batch plus petit
            test_input = torch.randn(1, 3, 256, 256)  # RÃ©solution rÃ©duite
            if torch.cuda.is_available():
                model = model.cuda()
                test_input = test_input.cuda()
            
            print(f"   ğŸ“Š Input test: {test_input.shape}")
            
            # Forward pass
            with torch.no_grad():
                output = model(test_input)
            
            print(f"   âœ… ModÃ¨le chargÃ© et testÃ© avec succÃ¨s!")
            print(f"   ğŸ“ˆ Sorties: {len(output)} tÃªtes de dÃ©tection")
            for i, out in enumerate(output):
                print(f"      P{i+2}: {out.shape}")
            
            return True
        else:
            print(f"   âŒ Fichier YAML non trouvÃ©: {yaml_path}")
            return False
            
    except Exception as e:
        print(f"   âŒ Erreur lors du test du modÃ¨le: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Fonction principale de test"""
    print("=" * 60)
    print("ğŸ§ª TEST RAPIDE - Modules GD OptimisÃ©s ADYOLOv5")
    print("=" * 60)
    
    # Information systÃ¨me
    print(f"ğŸ Python: {sys.version.split()[0]}")
    print(f"ğŸ”¥ PyTorch: {torch.__version__}")
    if torch.cuda.is_available():
        print(f"ğŸš€ CUDA: {torch.version.cuda}")
        print(f"ğŸ“± GPU: {torch.cuda.get_device_name()}")
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"ğŸ’¾ MÃ©moire GPU totale: {total_memory:.1f} GB")
    else:
        print("âš ï¸  CUDA non disponible, utilisation CPU")
    
    # Tests
    success = True
    
    # Test 1: Modules GD
    if not test_gd_modules():
        success = False
    
    # Test 2: Chargement modÃ¨le
    if not test_model_loading():
        success = False
    
    print("\n" + "=" * 60)
    if success:
        print("âœ… TOUS LES TESTS ONT RÃ‰USSI!")
        print("ğŸš€ Vous pouvez procÃ©der Ã  l'entraÃ®nement ADYOLOv5")
        print("\nCommandes suggÃ©rÃ©es:")
        print("  python main.py --model-size ad --memory-optimized")
        print("  ou")
        print("  python train_adyolo_optimized.py")
    else:
        print("âŒ CERTAINS TESTS ONT Ã‰CHOUÃ‰!")
        print("ğŸ”§ VÃ©rifiez les erreurs ci-dessus avant l'entraÃ®nement")
    print("=" * 60)

if __name__ == "__main__":
    main()
